{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#this probably doesn't run in the colab don't call this function\n",
        "#this runs in windows 11 with a cuda capable gpu\n",
        "\n",
        "def BERT_gpu_train():\n",
        "  #This code is modified from the tutorial included in the miniproject-3 handout to fetch attention matrices from the trained model\n",
        "\n",
        "  #ON WINDOWS 11 must set LongPathsEnabled=1:\n",
        "  #New-ItemProperty -Path \"HKLM:\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\" -Name \"LongPathsEnabled\" -Value 1 -PropertyType DWORD -Force\n",
        "\n",
        "  #this helps reduce memory pressure on the GPU\n",
        "  import os\n",
        "  os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"max_split_size_mb:512\"\n",
        "\n",
        "  #import stuff for the pretrained BERT model\n",
        "  import sys\n",
        "  import numpy as np\n",
        "  import random\n",
        "  import pandas as pd\n",
        "  import torch\n",
        "  #cannot use pytorch_pretrained_bert to output attention for some reason\n",
        "  from transformers import BertModel\n",
        "  from torch import nn\n",
        "  import datasets\n",
        "  from pytorch_pretrained_bert import BertTokenizer\n",
        "  from keras.utils import pad_sequences\n",
        "  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "  from torch.optim import Adam\n",
        "  from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\n",
        "  print(\"defining model...\")\n",
        "\n",
        "  #The pretrained BERT model with one fully connected layer appended to do classification\n",
        "  #extend the nn.Module class so we can add the sigmoid FC layer\n",
        "  class BertBinaryClassifier(nn.Module):\n",
        "      def __init__(self, dropout=0.1):\n",
        "          super(BertBinaryClassifier, self).__init__()\n",
        "          self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "          self.dropout=nn.Dropout(dropout)\n",
        "          self.linear = nn.Linear(768,1)\n",
        "          self.sigmoid=nn.Sigmoid()\n",
        "\n",
        "      def forward(self,tokens,masks=None):\n",
        "          out = self.bert(tokens, attention_mask=masks)\n",
        "          pooled_output = out.pooler_output\n",
        "          dropout_output=self.dropout(pooled_output)\n",
        "          linear_output=self.linear(dropout_output)\n",
        "          prob = self.sigmoid(linear_output)\n",
        "          return prob\n",
        "\n",
        "      #forward but output attention instead of logit\n",
        "      def forward_atten(self,tokens):\n",
        "          out = self.bert(tokens, attention_mask=None,output_attentions=True)\n",
        "          attentions = out.attentions\n",
        "          return attentions\n",
        "\n",
        "  print(\"setup gpu and prepare model...\")\n",
        "\n",
        "  #use the GPU\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "  #init the model and send it to the GPU\n",
        "  bert_clf = BertBinaryClassifier()\n",
        "  bert_clf = bert_clf.cuda()\n",
        "\n",
        "  print(\"loading data...\")\n",
        "\n",
        "  #load the IMDB movie reviews dataset\n",
        "  imdb = datasets.load_dataset(\"imdb\")\n",
        "\n",
        "  #process the imdb data according to the needs of the bert model\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
        "  train_tokens = list(map(lambda t: ['[CLS]']+tokenizer.tokenize(t)[:510]+['[SEP]'], imdb[\"train\"][\"text\"]))\n",
        "  test_tokens = list(map(lambda t: ['[CLS]']+tokenizer.tokenize(t)[:510]+['[SEP]'], imdb[\"test\"][\"text\"]))\n",
        "  train_token_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)),maxlen=512,truncating=\"post\",padding=\"post\",dtype=\"int\")\n",
        "  test_token_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)),maxlen=512,truncating=\"post\",padding=\"post\",dtype=\"int\")\n",
        "  train_y = np.array(imdb[\"train\"][\"label\"])\n",
        "  test_y = np.array(imdb[\"test\"][\"label\"])\n",
        "  test_orig_x = np.array(imdb[\"test\"][\"text\"])\n",
        "\n",
        "  #masks are important for BERT to prevent overfitting\n",
        "  train_masks = [[float(i>0) for i in ii] for ii in train_token_ids]\n",
        "  test_masks = [[float(i>0) for i in ii] for ii in test_token_ids]\n",
        "\n",
        "  #import a function to report precision,accuracy,recall...\n",
        "  from sklearn.metrics import classification_report\n",
        "\n",
        "  #hyperparamters\n",
        "  BATCH_SIZE = 3\n",
        "  EPOCHS = 2\n",
        "\n",
        "  print(\"preparing dataset...\")\n",
        "\n",
        "  #make our data into tensors\n",
        "  train_tokens_tensor = torch.tensor(train_token_ids)\n",
        "  train_y_tensor = torch.tensor(train_y.reshape(-1,1)).float()\n",
        "  test_tokens_tensor = torch.tensor(test_token_ids)\n",
        "  test_y_tensor = torch.tensor(test_y.reshape(-1,1)).float()\n",
        "  train_masks_tensor = torch.tensor(train_masks)\n",
        "  test_masks_tensor = torch.tensor(test_masks)\n",
        "\n",
        "  #wrap datasets with dataloaders\n",
        "  train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "  train_sampler = RandomSampler(train_dataset)\n",
        "  train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "  test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "  test_sampler = SequentialSampler(test_dataset)\n",
        "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "  #init an optimizer for the classifier (Adam w/ alpha=3e-06)\n",
        "  param_optimizer = list(bert_clf.sigmoid.named_parameters())\n",
        "  optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "  optimizer = Adam(bert_clf.parameters(), lr=3e-6)\n",
        "\n",
        "  #empty the GPU cache\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  #keep track of memory usage on GPU\n",
        "  print(str(torch.cuda.memory_allocated(device)/1000000)+\"M\")\n",
        "\n",
        "  print(\"train\")\n",
        "  #train the model\n",
        "  for epoch_num in range(EPOCHS):\n",
        "      #put model into training mode\n",
        "      bert_clf.train()\n",
        "      train_loss = 0\n",
        "      for step_num, batch_data in enumerate(train_dataloader):\n",
        "          token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "          print(str(torch.cuda.memory_allocated(device)/1000000)+\"M\")\n",
        "          logits = bert_clf(token_ids, masks)\n",
        "          loss_func = nn.BCELoss()\n",
        "          batch_loss = loss_func(logits, labels)\n",
        "          train_loss += batch_loss.item()\n",
        "          bert_clf.zero_grad()\n",
        "          batch_loss.backward()\n",
        "          clip_grad_norm_(parameters=bert_clf.parameters(),max_norm=1.0)\n",
        "          optimizer.step()\n",
        "          print(\"Epoch: \",epoch_num+1)\n",
        "          print(\"\\r\"+\"{0}/{1} loss: {2}\".format(step_num, len(train_dataset) // BATCH_SIZE, train_loss / (step_num + 1)))\n",
        "\n",
        "  print(\"done training.\")\n",
        "\n",
        "  #evaluate the model\n",
        "  bert_clf.eval()\n",
        "  bert_predicted = []\n",
        "  all_logits = []\n",
        "  with torch.no_grad():\n",
        "      for step_num, batch_data in enumerate(test_dataloader):\n",
        "          token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "          logits = bert_clf(token_ids, masks)\n",
        "          loss_func=nn.BCELoss()\n",
        "          loss = loss_func(logits, labels)\n",
        "          numpy_logits = logits.cpu().detach().numpy()\n",
        "          bert_predicted += list(numpy_logits[:,0] > 0.5)\n",
        "          all_logits += list(numpy_logits[:,0])\n",
        "          print(\"eval batch\",step_num, \"/\",len(train_dataset)//BATCH_SIZE)\n",
        "      print(classification_report(test_y,bert_predicted))\n",
        "\n",
        "\n",
        "\n",
        "  #get attention example\n",
        "  #instance_loader = DataLoader(test_dataset,sampler=test_sampler,batch_size=1)\n",
        "  #D = [i for i in enumerate(instance_loader)]\n",
        "  #token_ids, masks, labels = tuple(t.to(device) for t in D[0][1])\n",
        "  #attn = bert_clf.forward_atten(token_ids)\n",
        "  #text = imdb[\"test\"][\"text\"][0]\n",
        "  #tokens = test_tokens[0]\n",
        "\n",
        "  #jump into interactive shell to play with attention matrices and trained model\n",
        "  import code\n",
        "  v = globals().copy()\n",
        "  v.update(locals())\n",
        "  shell = code.InteractiveConsole(v)\n",
        "  shell.interact()\n",
        "\n",
        "  #               precision    recall  f1-score   support\n",
        "\n",
        "  #            0       0.96      0.90      0.93     12500\n",
        "  #            1       0.90      0.96      0.93     12500\n",
        "\n",
        "  #     accuracy                           0.93     25000\n",
        "  #    macro avg       0.93      0.93      0.93     25000\n",
        "  # weighted avg       0.93      0.93      0.93     25000\n",
        "\n",
        "  #interesting:\n",
        "  #1289 false positives\n",
        "  #494 false negatives\n",
        "\n",
        "  #0 true negative logit=0.0009 label=1\n",
        "  #12518 false negative logit = 0.0010 label=1\n",
        "\n",
        "  #longer training run:\n",
        "  #Output of run\n",
        "  #              precision    recall  f1-score   support\n",
        "  #\n",
        "  #            0       0.95      0.93      0.94     12500\n",
        "  #            1       0.93      0.95      0.94     12500\n",
        "  #\n",
        "  #     accuracy                           0.94     25000\n",
        "  #    macro avg       0.94      0.94      0.94     25000\n",
        "  # weighted avg       0.94      0.94      0.94     25000\n"
      ],
      "metadata": {
        "id": "2cnR3BwWCHDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NAIVE BAYES MODEL"
      ],
      "metadata": {
        "id": "JVoxRNuBClca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip train.zip -d \"train/\" #run this to unzip train"
      ],
      "metadata": {
        "id": "bDK7BFceouud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip test.zip -d \"test/\" #run this to unzip test"
      ],
      "metadata": {
        "id": "Q9D1DrnJpLgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll2lVEjOKnCK"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "\n",
        "class IMDBProcessor():\n",
        "    def __init__(self,verbose=1,prefix=None,suffix=None,word_freq_lower_limit=5,word_len_lower_limit=4,word_filter=str.isalpha,ngrams=[1]):\n",
        "        self.suffix=suffix\n",
        "        self.prefix=prefix\n",
        "        self.verbose = verbose\n",
        "        if self.verbose > 0:\n",
        "            print(\"processing...\")\n",
        "        self.ngrams = ngrams\n",
        "        self.word_filter = word_filter\n",
        "        self.word_len_lower_limit = word_len_lower_limit\n",
        "        neg_directory = 'train/neg'\n",
        "        pos_directory = 'train/pos'\n",
        "        neg_directory_test = 'test/neg'\n",
        "        pos_directory_test = 'test/pos'\n",
        "        self.vocab = dict()\n",
        "        self.pos_vocab = dict()\n",
        "        self.neg_vocab = dict()\n",
        "        self.neg_review_tokens = self._process_docs(neg_directory)\n",
        "        self.pos_review_tokens = self._process_docs(pos_directory)\n",
        "        self.neg_review_tokens_test = self._process_docs(neg_directory_test)\n",
        "        self.pos_review_tokens_test = self._process_docs(pos_directory_test)\n",
        "        self.pos_review_strings = self._imdb_strings(pos_directory_test)\n",
        "        self.neg_review_strings = self._imdb_strings(neg_directory_test)\n",
        "        word_set = set()\n",
        "        if self.verbose > 0:\n",
        "            print(\"building vocab...\")\n",
        "        for ts in self.neg_review_tokens+self.pos_review_tokens:\n",
        "            for t in ts:\n",
        "                word_set.add(t)\n",
        "        for word in word_set:\n",
        "            self.vocab[word]=0.0\n",
        "            self.neg_vocab[word] = 0.0\n",
        "            self.pos_vocab[word] = 0.0\n",
        "        for ts in self.neg_review_tokens:\n",
        "            for t in ts:\n",
        "                self.neg_vocab[t] += 1.0\n",
        "                self.vocab[t] += 1.0\n",
        "        for ts in self.pos_review_tokens:\n",
        "            for t in ts:\n",
        "                self.pos_vocab[t] += 1.0\n",
        "                self.vocab[t] += 1.0\n",
        "        low_freq = []\n",
        "        for word in self.vocab:\n",
        "            if self.vocab[word] < word_freq_lower_limit:\n",
        "                low_freq.append(word)\n",
        "        for word in low_freq:\n",
        "            self.vocab.pop(word)\n",
        "            self.pos_vocab.pop(word)\n",
        "            self.neg_vocab.pop(word)\n",
        "    def _ngrams(self, n, tokens):\n",
        "        return list(map(lambda l:' '.join(l),zip(*[tokens[n-i-1:len(tokens)-i] for i in reversed(range(n))])))\n",
        "    def _load_doc(self,filename):\n",
        "        file = open(filename, 'r')\n",
        "        text = file.read()\n",
        "        file.close()\n",
        "        return text\n",
        "    def _process_docs(self,directory):\n",
        "        tokens = []\n",
        "        c = 1\n",
        "        files = listdir(directory)\n",
        "        total = len(files)\n",
        "        for filename in files:\n",
        "            c += 1\n",
        "            if self.verbose > 1:\n",
        "                print(\"file\",c, \"/\",total)\n",
        "            if not filename.endswith(\".txt\"):\n",
        "                continue\n",
        "            path = directory + '/' + filename\n",
        "            doc = self._load_doc(path)\n",
        "            doc_tokens = doc.split(\" \")\n",
        "            if self.suffix is not None:\n",
        "                doc_tokens = doc_tokens[-self.suffix:]\n",
        "            elif self.prefix is not None:\n",
        "                doc_tokens = doc_tokens[:self.prefix]\n",
        "            doc_tokens = [''.join(filter(self.word_filter, token)).lower() for token in doc_tokens if len(token)>=self.word_len_lower_limit]\n",
        "            ngram_tokens = []\n",
        "            for n in self.ngrams:\n",
        "                ngrams = self._ngrams(n,doc_tokens)\n",
        "                for ngram in ngrams:\n",
        "                    ngram_tokens.append(ngram)\n",
        "            tokens.append(ngram_tokens)\n",
        "        return tokens\n",
        "    def _imdb_strings(self, directory):\n",
        "        strings = []\n",
        "        files = listdir(directory)\n",
        "        for filename in files:\n",
        "            if not filename.endswith(\".txt\"):\n",
        "                continue\n",
        "            path = directory + '/' + filename\n",
        "            doc = self._load_doc(path)\n",
        "            strings.append(doc)\n",
        "        return strings\n",
        "    def _process_string(self,string):\n",
        "        tokens = string.split(\" \")\n",
        "        tokens = [''.join(filter(str.isalpha, token)).lower() for token in tokens if len(token)>=self.word_len_lower_limit]\n",
        "        ngram_tokens = []\n",
        "        for n in self.ngrams:\n",
        "            ngrams = self._ngrams(n, tokens)\n",
        "            for ngram in ngrams:\n",
        "                ngram_tokens.append(ngram)\n",
        "        return ngram_tokens\n",
        "                \n",
        "class NaiveBayesBagged():\n",
        "    def __init__(self, processors, verbose=1,laplace=False):\n",
        "        self.verbose = verbose\n",
        "        self.processors = processors\n",
        "        self.prior_pos = len(processors[0].pos_review_tokens)/(len(processors[0].neg_review_tokens)+len(processors[0].pos_review_tokens))\n",
        "        self.prior_neg = 1-self.prior_pos\n",
        "        self.models = dict()\n",
        "        for ind,processor in enumerate(processors):\n",
        "            self.models[ind] = dict()\n",
        "            theta_pos = dict()\n",
        "            theta_neg = dict()\n",
        "            if verbose > 0:\n",
        "                print(\"building model... \"+str(ind))\n",
        "            for word in processor.vocab:\n",
        "                theta_pos[word] = 0.0\n",
        "                theta_neg[word] = 0.0\n",
        "            for word in processor.pos_vocab:\n",
        "                theta_pos[word] = processor.pos_vocab[word]\n",
        "            for word in processor.neg_vocab:\n",
        "                theta_neg[word] = processor.neg_vocab[word]\n",
        "            for word in processor.vocab:\n",
        "                if not laplace:\n",
        "                    theta_pos[word] /= processor.vocab[word]\n",
        "                    theta_neg[word] /= processor.vocab[word]\n",
        "                elif laplace:\n",
        "                    theta_pos[word] += 1\n",
        "                    theta_neg[word] += 1\n",
        "                    theta_pos[word] /= (processor.vocab[word]+2)\n",
        "                    theta_neg[word] /= (processor.vocab[word]+2)\n",
        "            self.models[ind][\"theta_pos\"]=theta_pos\n",
        "            self.models[ind][\"theta_neg\"]=theta_neg\n",
        "\n",
        "    def test_acc_pos(self):\n",
        "        if self.verbose > 0:\n",
        "            print(\"testing...\")\n",
        "        correct = 0.0\n",
        "        for review_ind in range(len(self.processors[0].pos_review_strings)):\n",
        "            av_prob_pos=0.0\n",
        "            av_prob_neg=0.0\n",
        "            for ind,processor in enumerate(self.processors):\n",
        "                prob_pos = self.prior_pos\n",
        "                prob_neg = self.prior_neg\n",
        "                theta_pos = self.models[ind][\"theta_pos\"]\n",
        "                theta_neg = self.models[ind][\"theta_neg\"]\n",
        "                tokens = processor.pos_review_tokens_test[review_ind]\n",
        "                for t in tokens:\n",
        "                    if t in processor.vocab:\n",
        "                        prob_pos*=theta_pos[t]\n",
        "                        prob_neg*=theta_neg[t]\n",
        "                norm = prob_pos+prob_neg\n",
        "                if norm == 0:\n",
        "                    norm = 1\n",
        "                prob_pos /= norm\n",
        "                prob_neg /= norm\n",
        "                av_prob_pos += prob_pos\n",
        "                av_prob_neg += prob_neg\n",
        "            av_prob_pos /= len(self.processors)\n",
        "            av_prob_neg /= len(self.processors)\n",
        "            if av_prob_pos >= av_prob_neg:\n",
        "                correct+=1\n",
        "        return correct / len(self.processors[0].pos_review_tokens)\n",
        "    def test_acc_neg(self):\n",
        "        if self.verbose > 0:\n",
        "            print(\"testing...\")\n",
        "        correct = 0.0\n",
        "        for review_ind in range(len(self.processors[0].neg_review_strings)):\n",
        "            av_prob_pos=0.0\n",
        "            av_prob_neg=0.0\n",
        "            for ind,processor in enumerate(self.processors):\n",
        "                prob_pos = self.prior_pos\n",
        "                prob_neg = self.prior_neg\n",
        "                theta_pos = self.models[ind][\"theta_pos\"]\n",
        "                theta_neg = self.models[ind][\"theta_neg\"]\n",
        "                tokens = processor.neg_review_tokens_test[review_ind]\n",
        "                for t in tokens:\n",
        "                    if t in processor.vocab:\n",
        "                        prob_pos*=theta_pos[t]\n",
        "                        prob_neg*=theta_neg[t]\n",
        "                norm = prob_pos+prob_neg\n",
        "                if norm == 0:\n",
        "                    norm = 1\n",
        "                prob_pos /= norm\n",
        "                prob_neg /= norm\n",
        "                av_prob_pos += prob_pos\n",
        "                av_prob_neg += prob_neg\n",
        "            av_prob_pos /= len(self.processors)\n",
        "            av_prob_neg /= len(self.processors)\n",
        "            if av_prob_pos <= av_prob_neg:\n",
        "                correct+=1\n",
        "        return correct / len(self.processors[0].neg_review_tokens)\n",
        "\n",
        "    def test_acc(self):\n",
        "        if self.verbose > 0:\n",
        "            print(\"testing...\")\n",
        "        correct = 0.0\n",
        "        for review_ind in range(len(self.processors[0].pos_review_strings)):\n",
        "            av_prob_pos=0.0\n",
        "            av_prob_neg=0.0\n",
        "            for ind,processor in enumerate(self.processors):\n",
        "                prob_pos = self.prior_pos\n",
        "                prob_neg = self.prior_neg\n",
        "                theta_pos = self.models[ind][\"theta_pos\"]\n",
        "                theta_neg = self.models[ind][\"theta_neg\"]\n",
        "                tokens = processor.pos_review_tokens_test[review_ind]\n",
        "                for t in tokens:\n",
        "                    if t in processor.vocab:\n",
        "                        prob_pos*=theta_pos[t]\n",
        "                        prob_neg*=theta_neg[t]\n",
        "                norm = prob_pos+prob_neg\n",
        "                if norm == 0:\n",
        "                    norm = 1\n",
        "                prob_pos /= norm\n",
        "                prob_neg /= norm\n",
        "                av_prob_pos += prob_pos\n",
        "                av_prob_neg += prob_neg\n",
        "            av_prob_pos /= len(self.processors)\n",
        "            av_prob_neg /= len(self.processors)\n",
        "            if av_prob_pos >= av_prob_neg:\n",
        "                correct+=1\n",
        "        for review_ind in range(len(self.processors[0].neg_review_strings)):\n",
        "            av_prob_pos\n",
        "            av_prob_neg\n",
        "            for ind,processor in enumerate(self.processors):\n",
        "                prob_pos = self.prior_pos\n",
        "                prob_neg = self.prior_neg\n",
        "                theta_pos = self.models[ind][\"theta_pos\"]\n",
        "                theta_neg = self.models[ind][\"theta_neg\"]\n",
        "                tokens = processor.neg_review_tokens_test[review_ind]\n",
        "                for t in tokens:\n",
        "                    if t in processor.vocab:\n",
        "                        prob_pos*=theta_pos[t]\n",
        "                        prob_neg*=theta_neg[t]\n",
        "                norm = prob_pos+prob_neg\n",
        "                if norm == 0:\n",
        "                    norm = 1\n",
        "                prob_pos /= norm\n",
        "                prob_neg /= norm\n",
        "                av_prob_pos += prob_pos\n",
        "                av_prob_neg += prob_neg\n",
        "            av_prob_pos /= len(self.processors)\n",
        "            av_prob_neg /= len(self.processors)\n",
        "            if av_prob_pos <= av_prob_neg:\n",
        "                correct+=1\n",
        "        return correct / len(self.processors[0].pos_review_tokens+self.processors[0].neg_review_tokens)\n",
        "\n",
        "class NaiveBayes():\n",
        "    def __init__(self, processor, verbose=1,laplace=False):\n",
        "        self.verbose = verbose\n",
        "        self.processor = processor\n",
        "        self.prior_pos = len(processor.pos_review_tokens)/(len(processor.neg_review_tokens)+len(processor.pos_review_tokens))\n",
        "        self.prior_neg = 1-self.prior_pos\n",
        "        self.theta_pos = dict()\n",
        "        self.theta_neg = dict()\n",
        "        if verbose > 0:\n",
        "            print(\"building model...\")\n",
        "        for word in processor.vocab:\n",
        "            self.theta_pos[word] = 0.0\n",
        "            self.theta_neg[word] = 0.0\n",
        "        for word in processor.pos_vocab:\n",
        "            self.theta_pos[word] = processor.pos_vocab[word]\n",
        "        for word in processor.neg_vocab:\n",
        "            self.theta_neg[word] = processor.neg_vocab[word]\n",
        "        for word in processor.vocab:\n",
        "            if not laplace:\n",
        "                self.theta_pos[word] /= processor.vocab[word]\n",
        "                self.theta_neg[word] /= processor.vocab[word]\n",
        "            elif laplace:\n",
        "                self.theta_pos[word] += 1\n",
        "                self.theta_neg[word] += 1\n",
        "                self.theta_pos[word] /= (processor.vocab[word]+2)\n",
        "                self.theta_neg[word] /= (processor.vocab[word]+2)\n",
        "    def predict_string(self, review):\n",
        "        prob_pos = self.prior_pos\n",
        "        prob_neg = self.prior_neg\n",
        "        for word in self.processor._process_string(review):\n",
        "            if word in self.processor.vocab:\n",
        "                prob_pos*=self.theta_pos[word]\n",
        "                prob_neg*=self.theta_neg[word]\n",
        "        if prob_pos>prob_neg:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def test_acc(self):\n",
        "        if self.verbose > 0:\n",
        "            print(\"testing...\")\n",
        "        correct = 0.0\n",
        "        incorrects = []\n",
        "        for tokens,review in zip(self.processor.pos_review_tokens_test,self.processor.pos_review_strings):\n",
        "            prob_pos = self.prior_pos\n",
        "            prob_neg = self.prior_neg\n",
        "            for t in tokens:\n",
        "                if t in self.processor.vocab:\n",
        "                    prob_pos*=self.theta_pos[t]\n",
        "                    prob_neg*=self.theta_neg[t]\n",
        "            norm = prob_pos+prob_neg\n",
        "            if norm == 0:\n",
        "                norm = 1\n",
        "            prob_pos /= norm\n",
        "            prob_neg /= norm\n",
        "            if prob_pos>prob_neg:\n",
        "                correct +=1\n",
        "            else:\n",
        "                incorrects.append([review, 1, prob_pos,prob_neg])\n",
        "        for tokens,review in zip(self.processor.neg_review_tokens_test,self.processor.neg_review_strings):\n",
        "            prob_pos = self.prior_pos\n",
        "            prob_neg = self.prior_neg\n",
        "            for t in tokens:\n",
        "                if t in self.processor.vocab:\n",
        "                    prob_pos*=self.theta_pos[t]\n",
        "                    prob_neg*=self.theta_neg[t]\n",
        "            norm = prob_pos+prob_neg\n",
        "            if norm == 0:\n",
        "                norm = 1\n",
        "            prob_pos /= norm\n",
        "            prob_neg /= norm\n",
        "            if prob_pos<prob_neg:\n",
        "                correct +=1\n",
        "            else:\n",
        "                incorrects.append([review,0,prob_pos,prob_neg])\n",
        "        return correct / len(self.processor.pos_review_tokens_test+self.processor.neg_review_tokens_test), incorrects\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#models\n",
        "#nb_best = NaiveBayes(IMDBProcessor(word_freq_lower_limit=1,word_len_lower_limit=1,ngrams=[2,4]), laplace=True)  #acc=0.883\n",
        "#nb = NaiveBayes(IMDBProcessor(verbose=0,word_freq_lower_limit=1,word_len_lower_limit=1,ngrams=[1]), verbose=0,laplace=True) \n",
        "#acc1,inc1 = nb.test_acc()\n",
        "#nb2 = NaiveBayes(IMDBProcessor(verbose=0,word_freq_lower_limit=1,word_len_lower_limit=1,ngrams=[2,4]), verbose=0,laplace=True)\n",
        "#acc2,inc2 = nb2.test_acc()\n",
        "#print(acc1)\n",
        "#print(acc2)\n",
        "#nb_bag_best = NaiveBayesBagged([IMDBProcessor(verbose=1,word_freq_lower_limit=1,word_len_lower_limit=1,ngrams=[1]),\n",
        "#                           IMDBProcessor(verbose=1,word_freq_lower_limit=1,word_len_lower_limit=1,ngrams=[2,4])],\n",
        "#                           laplace=True)\n",
        "#nb_bag_best = NaiveBayesBagged([IMDBProcessor(verbose=1,word_freq_lower_limit=2,word_len_lower_limit=4,ngrams=[1]),\n",
        "#                           IMDBProcessor(verbose=1,word_freq_lower_limit=1,word_len_lower_limit=1,ngrams=[3]),\n",
        "#                           IMDBProcessor(verbose=1,word_freq_lower_limit=1,word_len_lower_limit=1,ngrams=[2,4])],\n",
        "#                           laplace=True)\n",
        "\n",
        "#best bagged model I could find\n",
        "nb = NaiveBayesBagged([IMDBProcessor(verbose=1,suffix=25,word_freq_lower_limit=2,word_len_lower_limit=5,ngrams=[1]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[3]),\n",
        "               IMDBProcessor(verbose=1,suffix=50,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[1,3]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[2,4])],laplace=True)\n",
        "print(nb.test_acc())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPE1pau7pb1r",
        "outputId": "4b04ec1b-05ea-4241-961b-b74505599f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing...\n",
            "building vocab...\n",
            "processing...\n",
            "building vocab...\n",
            "processing...\n",
            "building vocab...\n",
            "processing...\n",
            "building vocab...\n",
            "building model... 0\n",
            "building model... 1\n",
            "building model... 2\n",
            "building model... 3\n",
            "testing...\n",
            "0.90236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "12500-(nb.test_acc_pos()*12500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpUsJnZ4sNou",
        "outputId": "ed34b2dc-408c-4557-9f29-c7fef47b42b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1477.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "12500-(nb.test_acc_neg()*12500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m6GafKouOEQ",
        "outputId": "ede6d921-ff97-4ad7-d642-1f326b133559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1274.0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(nb.processors[0].vocab)+len(nb.processors[1].vocab)+len(nb.processors[2].vocab)+len(nb.processors[3].vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBmZvDDa9vi0",
        "outputId": "a00488b0-f62e-4bb6-8e56-c94a5b431eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1305704"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hill climb to find a good combination\n",
        "\n",
        "#best so far\n",
        "#8,16,17,4\n",
        "\n",
        "processors =  [IMDBProcessor(verbose=1,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[1]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=5,word_len_lower_limit=3,ngrams=[1]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=10,word_len_lower_limit=5,ngrams=[1]),\n",
        "               IMDBProcessor(verbose=1,prefix=25,word_freq_lower_limit=2,word_len_lower_limit=5,ngrams=[1]),\n",
        "               IMDBProcessor(verbose=1,suffix=25,word_freq_lower_limit=2,word_len_lower_limit=5,ngrams=[1]),\n",
        "               IMDBProcessor(verbose=1,suffix=10,word_freq_lower_limit=2,word_len_lower_limit=5,ngrams=[1]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[2]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=5,word_len_lower_limit=1,ngrams=[2]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[3]),\n",
        "               IMDBProcessor(verbose=1,prefix=25,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[3]),\n",
        "               IMDBProcessor(verbose=1,suffix=25,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[3]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=5,word_len_lower_limit=1,ngrams=[3]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=10,word_len_lower_limit=1,ngrams=[3]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[2,3]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[1,3]),\n",
        "               IMDBProcessor(verbose=1,prefix=50,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[1,3]),\n",
        "               IMDBProcessor(verbose=1,suffix=50,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[1,3]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[2,4]),\n",
        "               IMDBProcessor(verbose=1,prefix=25,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[2,4]),\n",
        "               IMDBProcessor(verbose=1,suffix=25,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[2,4]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[1,2]),\n",
        "               IMDBProcessor(verbose=1,suffix=50,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[1,2]),\n",
        "               IMDBProcessor(verbose=1,prefix=50,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[1,2]),\n",
        "               IMDBProcessor(verbose=1,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[1,4]),\n",
        "               IMDBProcessor(verbose=1,prefix=50,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[1,4]),\n",
        "               IMDBProcessor(verbose=1,prefix=25,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[1,4]),\n",
        "               IMDBProcessor(verbose=1,prefix=10,word_freq_lower_limit=2,word_len_lower_limit=1,ngrams=[1,4]),\n",
        "               ]\n",
        "\n",
        "\n",
        "import random\n",
        "rand_ind = lambda l: random.sample(range(len(l)),1)[0]\n",
        "#hill climb search for 50 iterations\n",
        "proc_inds = [4,8,16,17]\n",
        "saved_results = [[0,proc_inds]]\n",
        "for _ in range(100):\n",
        "    proc_inds = max(saved_results,key=lambda r:r[0])[1]\n",
        "    for _ in range(5):#5 steps from current max\n",
        "        proc_inds[rand_ind(proc_inds)] = rand_ind(processors)#swap one\n",
        "        procs = []\n",
        "        for i in proc_inds:\n",
        "            procs.append(processors[i])\n",
        "        nb = NaiveBayesBagged(procs,laplace=True)\n",
        "        acc = nb.test_acc()\n",
        "        print(acc)\n",
        "        saved_results.append([acc,proc_inds[:]])\n",
        "print(max(saved_results,key=lambda r:r[0]))\n",
        "\n",
        "        \n",
        "# 12x12 -> 512x512\n",
        "\n",
        "\n",
        "#no laplace\n",
        "#               precision    recall  f1-score   support\n",
        "# \n",
        "#            0       0.76      0.82      0.79     12500\n",
        "#            1       0.81      0.74      0.77     12500\n",
        "# \n",
        "#     accuracy                           0.78     25000\n",
        "#    macro avg       0.78      0.78      0.78     25000\n",
        "# weighted avg       0.78      0.78      0.78     25000\n",
        "\n",
        "#laplace \n",
        "#               precision    recall  f1-score   support\n",
        "# \n",
        "#            0       0.82      0.84      0.83     12500\n",
        "#            1       0.84      0.82      0.83     12500\n",
        "# \n",
        "#     accuracy                           0.83     25000\n",
        "#    macro avg       0.83      0.83      0.83     25000\n",
        "# weighted avg       0.83      0.83      0.83     25000\n",
        "\n",
        "\n",
        "#freq 1 len 1 ngram [2]\n",
        "#acc: 0.87208\n",
        "#vocab size: 1452767\n",
        "#freq 1 len 1 ngram [3]\n",
        "#acc: 0.8764\n",
        "#vocab size: 3657592\n",
        "#freq 1 len 1 ngram [2, 4]\n",
        "#acc: 0.8834\n",
        "#vocab size: 6485060\n",
        "#freq 1 len 1 ngram [2, 3]\n",
        "#acc: 0.87556\n",
        "#vocab size: 5110359\n",
        "#freq 1 len 1 ngram [3, 4]\n",
        "#acc: 0.87704\n",
        "#vocab size: 8689885\n",
        "#freq 1 len 2 ngram [2]\n",
        "#acc: 0.87192\n",
        "#vocab size: 1506216\n",
        "#freq 1 len 2 ngram [3]\n",
        "#acc: 0.87356\n",
        "#vocab size: 3691440\n",
        "#freq 1 len 2 ngram [2, 4]\n",
        "#acc: 0.88088\n",
        "#vocab size: 6426719\n",
        "#freq 1 len 2 ngram [2, 3]\n",
        "#acc: 0.87812\n",
        "#vocab size: 5197656\n",
        "#freq 1 len 2 ngram [3, 4]\n",
        "#acc: 0.87096\n",
        "#vocab size: 8611943\n",
        "#freq 1 len 3 ngram [2]\n",
        "#acc: 0.87156\n",
        "#vocab size: 1637887\n",
        "#freq 1 len 3 ngram [3]\n",
        "#acc: 0.84924\n",
        "#vocab size: 3595510\n",
        "#freq 1 len 3 ngram [2, 4]\n",
        "#acc: 0.8768\n",
        "#vocab size: 6022243\n",
        "#freq 1 len 3 ngram [2, 3]\n",
        "#acc: 0.8792\n",
        "#vocab size: 5233397\n",
        "#freq 1 len 3 ngram [3, 4]\n",
        "#acc: 0.8456\n",
        "#vocab size: 7979866\n",
        "#freq 5 len 1 ngram [2]\n",
        "#acc: 0.87284\n",
        "#vocab size: 130106\n",
        "#freq 5 len 1 ngram [3]\n",
        "#acc: 0.86532\n",
        "#vocab size: 108883\n",
        "#freq 5 len 1 ngram [2, 4]\n",
        "#acc: 0.8792\n",
        "#vocab size: 170239\n",
        "#freq 5 len 1 ngram [2, 3]\n",
        "#acc: 0.88036\n",
        "#vocab size: 238989\n",
        "#freq 5 len 1 ngram [3, 4]\n",
        "#acc: 0.862\n",
        "#vocab size: 149016\n",
        "#freq 5 len 2 ngram [2]\n",
        "#acc: 0.87276\n",
        "#vocab size: 129550\n",
        "#freq 5 len 2 ngram [3]\n",
        "#acc: 0.85888\n",
        "#vocab size: 95728\n",
        "#freq 5 len 2 ngram [2, 4]\n",
        "#acc: 0.8776\n",
        "#vocab size: 161177\n",
        "#freq 5 len 2 ngram [2, 3]\n",
        "#acc: 0.8796\n",
        "#vocab size: 225278\n",
        "#freq 5 len 2 ngram [3, 4]\n",
        "#acc: 0.8544\n",
        "#vocab size: 127355\n",
        "#freq 5 len 3 ngram [2]\n",
        "#acc: 0.87176\n",
        "#vocab size: 114038\n",
        "#freq 5 len 3 ngram [3]\n",
        "#acc: 0.82244\n",
        "#vocab size: 57964\n",
        "#freq 5 len 3 ngram [2, 4]\n",
        "#acc: 0.8738\n",
        "#vocab size: 126285\n",
        "#freq 5 len 3 ngram [2, 3]\n",
        "#acc: 0.87548\n",
        "#vocab size: 172002\n",
        "#freq 5 len 3 ngram [3, 4]\n",
        "#acc: 0.81732\n",
        "#vocab size: 70211\n",
        "#freq 10 len 1 ngram [2]\n",
        "#acc: 0.86812\n",
        "#vocab size: 63733\n",
        "#freq 10 len 1 ngram [3]\n",
        "#acc: 0.85304\n",
        "#vocab size: 42351\n",
        "#freq 10 len 1 ngram [2, 4]\n",
        "#acc: 0.87276\n",
        "#vocab size: 75934\n",
        "#freq 10 len 1 ngram [2, 3]\n",
        "#acc: 0.87596\n",
        "#vocab size: 106084\n",
        "#freq 10 len 1 ngram [3, 4]\n",
        "#acc: 0.8474\n",
        "#vocab size: 54552\n",
        "#freq 10 len 2 ngram [2]\n",
        "#acc: 0.86956\n",
        "#vocab size: 62444\n",
        "#freq 10 len 2 ngram [3]\n",
        "#acc: 0.84268\n",
        "#vocab size: 36216\n",
        "#freq 10 len 2 ngram [2, 4]\n",
        "#acc: 0.8728\n",
        "#vocab size: 71759\n",
        "#freq 10 len 2 ngram [2, 3]\n",
        "#acc: 0.87504\n",
        "#vocab size: 98660\n",
        "#freq 10 len 2 ngram [3, 4]\n",
        "#acc: 0.836\n",
        "#vocab size: 45531\n",
        "#freq 10 len 3 ngram [2]\n",
        "#acc: 0.8664\n",
        "#vocab size: 52468\n",
        "#freq 10 len 3 ngram [3]\n",
        "#acc: 0.79216\n",
        "#vocab size: 19977\n",
        "#freq 10 len 3 ngram [2, 4]\n",
        "#acc: 0.8672\n",
        "#vocab size: 55591\n",
        "#freq 10 len 3 ngram [2, 3]\n",
        "#acc: 0.86792\n",
        "#vocab size: 72445\n",
        "#freq 10 len 3 ngram [3, 4]\n",
        "#acc: 0.78812\n",
        "#vocab size: 23100\n"
      ],
      "metadata": {
        "id": "llqG20AXqu89"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}